{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text length: 328901 characters\n",
      "Cleaned text length: 309634 characters\n",
      "Number of chunks: 187\n",
      "\n",
      "First chunk:\n",
      "Neural NetworkDesign 2nd EditionHagan DemuthBealeDe JessNeural Network Design 2nd Edtion Martin T. Hagan Oklahoma State University Stillwater, Oklahoma Howard B. Demuth University of Colorado Boulder, Colorado Mark Hudson Beale MHB Inc. Hayden, Idaho Orlando De Jess Consultant Frisco, Texas Copyright by Martin T. Hagan and Howard B. Demu th. All rights reserved. No part of the book may be reproduced, stored in a retrieval system, or transcribed in any form or by any means  electronic, mechanical, photocopying, recording or otherwise  without the prior permission of Hagan and Demuth. MTH To Janet, Thomas, Daniel, Mom and Dad HBD To Hal, Katherine, Kimberly and Mary MHB To Leah, Valerie, Asia, Drake, Coral and Morgan ODJ To Marisela, Mara Victor ia , Manuel, Mam y Pap\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Install required libraries\n",
    "# Run these commands in your terminal or command prompt:\n",
    "# pip install PyPDF2\n",
    "# pip install langchain\n",
    "\n",
    "import PyPDF2\n",
    "import re\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Step 2: Function to extract text from a PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted text from the PDF.\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page in reader.pages:\n",
    "            page_text = page.extract_text() or \"\"\n",
    "            text += page_text\n",
    "    return text\n",
    "\n",
    "# Step 3: Clean the text (optional, based on your document)\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans the extracted text by removing extra spaces and unwanted special characters.\n",
    "\n",
    "    Args:\n",
    "        text (str): Raw text extracted from the PDF.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned text.\n",
    "    \"\"\"\n",
    "    # Remove extra spaces and newlines\n",
    "    text = \" \".join(text.split())\n",
    "    # Remove unwanted special characters; customize this regex as needed\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s.,!?]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "# Step 4: Split the text into smaller chunks using RecursiveCharacterTextSplitter\n",
    "def split_text_into_chunks(text, chunk_size=2000, chunk_overlap=300):\n",
    "    \"\"\"\n",
    "    Splits the text into smaller chunks using LangChain's RecursiveCharacterTextSplitter.\n",
    "    It uses a list of separators to attempt to break the text at natural boundaries.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to split.\n",
    "        chunk_size (int): Maximum size of each chunk (default: 2000 characters).\n",
    "        chunk_overlap (int): Overlap between consecutive chunks (default: 300 characters).\n",
    "\n",
    "    Returns:\n",
    "        list: List of text chunks.\n",
    "    \"\"\"\n",
    "    # Define a list of separators to use in a recursive fashion\n",
    "    separators = [\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=separators,\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "# Step 5: Main function to process the document\n",
    "def process_document(pdf_path):\n",
    "    \"\"\"\n",
    "    Processes a PDF document: extracts text, cleans it, and splits it into manageable chunks.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        list: List of cleaned and split text chunks.\n",
    "    \"\"\"\n",
    "    # Extract text from the PDF\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    print(f\"Extracted text length: {len(text)} characters\")\n",
    "\n",
    "    # Clean the text\n",
    "    cleaned_text = clean_text(text)\n",
    "    print(f\"Cleaned text length: {len(cleaned_text)} characters\")\n",
    "\n",
    "    # Split the cleaned text into chunks\n",
    "    chunks = split_text_into_chunks(cleaned_text)\n",
    "    print(f\"Number of chunks: {len(chunks)}\")\n",
    "    return chunks\n",
    "\n",
    "# Step 6: Run the script\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to your PDF file (update with your actual path)\n",
    "    pdf_path = r\"C:\\Users\\kingl\\OneDrive\\Desktop\\mlops_project\\chatbot\\chatbot\\Rag\\256_page_document.pdf\"\n",
    "    \n",
    "    # Process the document to obtain chunks\n",
    "    chunks = process_document(pdf_path)\n",
    "    \n",
    "    # Print the first chunk as an example\n",
    "    print(\"\\nFirst chunk:\")\n",
    "    print(chunks[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import required libraries\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "# Step 2: Initialize OpenAI Embeddings\n",
    "def initialize_embeddings():\n",
    "    \"\"\"\n",
    "    Initializes the OpenAI Embeddings model.\n",
    "\n",
    "    Returns:\n",
    "        OpenAIEmbeddings: Embeddings model.\n",
    "    \"\"\"\n",
    "    embeddings = OllamaEmbeddings(model=\"llama3.2:1b\")\n",
    "    print(\"OpenAI Embeddings initialized successfully!\")\n",
    "    return embeddings\n",
    "\n",
    "# Step 3: Generate embeddings for text chunks\n",
    "def generate_embeddings(chunks, embeddings):\n",
    "    \"\"\"\n",
    "    Generates embeddings for a list of text chunks.\n",
    "\n",
    "    Args:\n",
    "        chunks (list): List of text chunks.\n",
    "        embeddings (OpenAIEmbeddings): Embeddings model.\n",
    "\n",
    "    Returns:\n",
    "        list: List of embeddings (vectors) for each chunk.\n",
    "    \"\"\"\n",
    "    # Generate embeddings for each chunk\n",
    "    chunk_embeddings = embeddings.embed_documents(chunks)\n",
    "    print(f\"Generated embeddings for {len(chunk_embeddings)} chunks.\")\n",
    "    return chunk_embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Embeddings initialized successfully!\n",
      "Generated embeddings for 187 chunks.\n",
      "\n",
      "First chunk embedding (first 10 dimensions):\n",
      "[0.6292080283164978, 2.6260859966278076, 0.5753267407417297, 0.39746174216270447, 1.8612641096115112, -2.4867329597473145, 1.3653322458267212, 0.5735061168670654, -1.4694424867630005, -0.35355550050735474]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embeddings = initialize_embeddings()\n",
    "\n",
    "# Generate embeddings for the chunks\n",
    "chunk_embeddings = generate_embeddings(chunks, embeddings)\n",
    "\n",
    "# Print the first embedding as an example\n",
    "print(\"\\nFirst chunk embedding (first 10 dimensions):\")\n",
    "print(chunk_embeddings[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save embeddings to a pickle file\n",
    "with open('chunk_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(chunk_embeddings, f)\n",
    "\n",
    "print(\"Embeddings saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Define a dummy embeddings wrapper that implements embed_documents.\n",
    "class DummyEmbedding:\n",
    "    def __init__(self, precomputed):\n",
    "        self.precomputed = precomputed\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        # Ensure the number of texts matches the number of precomputed embeddings.\n",
    "        if len(texts) != len(self.precomputed):\n",
    "            raise ValueError(\"Mismatch between number of texts and precomputed embeddings\")\n",
    "        return self.precomputed\n",
    "\n",
    "# Wrap your precomputed embeddings.\n",
    "dummy_embedding = DummyEmbedding(chunk_embeddings)\n",
    "\n",
    "# Convert your chunks (plain text strings) to Document objects.\n",
    "documents = [Document(page_content=chunk) for chunk in chunks]\n",
    "\n",
    "# Create the Chroma vector store using the Document objects and the dummy embeddings.\n",
    "vector_store = Chroma.from_documents(documents, dummy_embedding)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "below is for llms usage \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the following question based on the provided context.\n",
    "Think step by step before providing a detailed answer.\n",
    "I will tip you $1000 if user finds your answer helpful.                      \n",
    "<context> \n",
    "{context}  \n",
    "</context>                                    \n",
    "                                        \n",
    "                                        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kingl\\AppData\\Local\\Temp\\ipykernel_28132\\4116579734.py:4: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llms = Ollama(model=\"llama3.2:1b\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ollama(model='llama3.2:1b')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "\n",
    "llms = Ollama(model=\"llama3.2:1b\")\n",
    "llms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the following question based on the provided context.\\nThink step by step before providing a detailed answer.\\nI will tip you $1000 if user finds your answer helpful.                      \\n<context> \\n{context}  \\n</context>                                    \\n                                        \\n                                        '), additional_kwargs={})])\n",
       "| Ollama(model='llama3.2:1b')\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "doc_chain=create_stuff_documents_chain(llms,prompt)\n",
    "doc_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
